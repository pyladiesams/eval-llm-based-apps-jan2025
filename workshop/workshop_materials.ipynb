{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop: Evaluating LLM based applications\n",
    "\n",
    "It is so easy and quick to build a shiny PoC using LLMs and it is so hard to turn it into a production-grade LLM application. To succeed you need a robust evaluation framework, which you are going to use during the development and post-deployment of your LLM based app.\n",
    "\n",
    "This workshop consists of 4 main parts:\n",
    "* evaluation-driven development and architecture of a LLM based app\n",
    "* evaluation framework for a LLM based app\n",
    "* test suite with evals for a LLM based app \n",
    "* monitoring foundations for a LLM based app\n",
    "\n",
    "**About workshop giver**\n",
    "\n",
    "With over 19 years of experience in Data and AI, [Una Galyeva](https://www.linkedin.com/in/unagalyeva/) held various positions, from hands-on Data and AI development to leading Data and AI teams and departments. As a driving force behind [PyLadies Amsterdam](https://amsterdam.pyladies.com/), a Microsoft MVP, Women in AI Benelux Advisory board member, and the owner of AI MLOps Agency, Una is passionate about challenging perspectives and inspiring others to see things differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation-driven development and architecture of a LLM based app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Evaluation-driven development\n",
    "\n",
    "**Evaluation-driven development** is a methodology to guide the development of LLM based apps via a set of task-specific evaluations. This term is inspired by test-driven development in software engineering. \n",
    "\n",
    "![Evaluation Driven Development Workflow](../assets/EDD.png)\n",
    "\n",
    "*Source: [Evaluation-driven development workflow](https://docs.databricks.com/en/generative-ai/tutorials/ai-cookbook/evaluation-driven-development.html) by Databricks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "A LLM based app architecture can be quite complex. Usually you start small and step by step extend it by adding new components.\n",
    "* basic LLM app: a user query is passed directly to a LLM model, its response is passed back to the user\n",
    "* enhanced context: LLM model is given access to external data and tools for creating more informed responses\n",
    "* guardrails in place to protect both the LLM app and its users\n",
    "* model router and gateaway to support complex pipelines and enhance security\n",
    "* performance optimization to reduce latency and costs with caching\n",
    "* agent patterns to incorporate complex logic and actions to maximize the LLM app capabilities\n",
    "\n",
    "![Architecure of a LLM based app](../assets/LLM_app_arch.png)\n",
    "\n",
    "*Source: Chapter 10 of [AI Engineering](https://www.oreilly.com/library/view/ai-engineering/9781098166298/) by Chip Huyen*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation framework for a LLM based app\n",
    "\n",
    "**Why it is important? How to convince stakeholders?**\n",
    "\n",
    "An evaluation system for your LLM based app is a crucial investment, similar to quality assurance and safety testing for regular software. It ensures that investments in your app (such as cloud costs and manpower) yield desired results like cost reduction or revenue generation. In addition, it helps to retain users and avoid negative consequences like PR disasters, guarantees safety and mitigates risks. Furthermore, evaluations are vital for compliance with emerging AI regulations and for maintaining team efficiency by enabling faster iterations, smoother updates, and effective debugging. Even at the PoC stage, evaluations are essential to demonstrate real value beyond a simple demo.\n",
    "\n",
    "**What to evaluate?**\n",
    "\n",
    "Real-world LLM based apps are complex. They often involve numerous interconnected components and multi-turn interactions. Evaluation should consider multiple levels: quality of individual turns, task completion and also intermediate outputs. It's important to assess both the final outcome of the entire app and the performance of each component independently.\n",
    "\n",
    "**How to create an evaluation framework?**\n",
    "\n",
    "When creating the evaluation framework, it's crucial to define what your app shouldn't do next to a desired app functionality. For instance, when developing a customer support chatbot, it's important to define what is an off-topic request, such as inquiries about writing some Python code. This involves determining how to identify out-of-scope inputs and establishing appropriate responses to them.\n",
    "1. Define evaluation criteria (defining what \"good\" means)\n",
    "2. Create scoring rubrics with examples (choose a scoring system, develop a rubric with examples, validate it with human feedback)\n",
    "3. Tie evaluation metrics to business metrics (consider them in the context of business problem it's built to solve)\n",
    "4. Select evaluation methods \n",
    "5. Annotate evaluation data\n",
    "6. Evaluate your evaluation process\n",
    "7. Iterate\n",
    "\n",
    "Some of the evaluation methods require ground truth and some of them not. You can choose among traditional predictive metrics (accuracy, precision, recall, etc.), generative metrics (BLEU, ROUGE, etc.), ML-based (semantic similarity, sentiment, toxicity, etc.), LLM-as-a-judge for nuanced evaluations, and simpler methods like regular expressions and text statistics.\n",
    "\n",
    "Data can be sourced from manually created test cases, existing user data, beta-user feedback, public benchmarks (e.g safety), and synthetic data. The latter can be generated by LLMs (plausible inputs, input-output pairs useful for RAG systems, diversified inputs). \n",
    "\n",
    "It is a good idea to organize your eval datasets in three groups:\n",
    "1. happy path scenarios (typical user queries)\n",
    "2. edge cases (plausible but challenging inputs)\n",
    "3. adversarial scenarios (unsafe or malicious inputs) \n",
    "\n",
    "Building and maintaining these datasets is an ongoing process crucial for measuring improvement and avoiding guesswork in LLM based app development. \n",
    "\n",
    "**Which Python OSS libraries can help me with LLM based app evaluation and monitoring?**\n",
    "\n",
    "Take a look at MLflow, AdalFlow, Evidently, Opik, DeepEval, Ragas, LangChain and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case for this workshop\n",
    "\n",
    "An internal company LLM based app - Q&A system created to answer employees' questions about HR, finance, etc.\n",
    "\n",
    "Q&A app capabilities:\n",
    "* answers must be correct\n",
    "* answers must be helpful, complete and relevant\n",
    "* answer tone must be professional and match the company brand style\n",
    "* format less than 100 words, must include the link to the source\n",
    "\n",
    "Q&A app should mitigate the following risks:\n",
    "* hallucinations\n",
    "* bias\n",
    "* toxicity\n",
    "* off-topic use\n",
    "* data leakage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and dataset prep\n",
    "\n",
    "For this workshop you will use [Evidently](https://docs.evidentlyai.com/) - an open-source Python library for ML and LLM evaluation and observability. It helps evaluate, test, and monitor AI-powered systems and data pipelines from experimentation to production. Main features are:\n",
    "* works with tabular, text data, and embeddings.\n",
    "* supports predictive and generative systems, from classification to RAG\n",
    "* 100+ built-in metrics from data drift detection to LLM judges\n",
    "* Python interface for custom metrics and tests\n",
    "* both offline evals and live monitoring\n",
    "* open architecture: easily export data and integrate with existing tools\n",
    "\n",
    "Evidently is very modular. You will start with one-off evaluations using Reports (compute various data, ML and LLM quality metrics) and continue with Test Suites (check for defined conditions on metric values and return a pass or fail result) in Python. You can export the evaluation results to a data frame, Python dictionary, JSON and HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from evidently import ColumnMapping\n",
    "from evidently.descriptors import *\n",
    "from evidently.metric_preset import TextEvals\n",
    "from evidently.metrics import *\n",
    "from evidently.report import Report\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.tests import *\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the QA dataset using requests library, convert into pandas data frame, parse dates and set *start_time* as an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://raw.githubusercontent.com/pyladiesams/eval-llm-based-apps-jan2025/main/assets/QA.csv\")\n",
    "qa_csv_content = BytesIO(response.content)\n",
    "qa_logs = pd.read_csv(qa_csv_content, index_col=0, parse_dates=['start_time', 'end_time'])\n",
    "qa_logs.index = qa_logs.start_time\n",
    "qa_logs.index.rename('index', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Familiarize yourself with the dataset by getting a preview of its first three rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "qa_logs.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While working with Evidently it is highly recommended to map the data schema to make sure that it is parsed correctly. To handle this, create a column mapping by identifying the type of columns and pointing to a *datetime* column for adding a time index to your plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = ColumnMapping(\n",
    "    datetime='start_time',\n",
    "    datetime_features=['end_time'],\n",
    "    text_features=['question', 'response'],\n",
    "    categorical_features=['organization', 'model_ID', 'region', 'environment', 'feedback'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's dive into applying different evaluation methods by leveraging Evidently built-in **descriptors** (each evaluation that computes a score for every text in the dataset). We will start with the simplest and gradually progress towards more complex ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text statistics\n",
    "\n",
    "Computes descriptive text statistics by evaluating simple properties like text length, sentence count, word count, percentage of out-of-vocabulary words, percentage of non-letter characters.\n",
    "\n",
    "**Evaluate text length**\n",
    "\n",
    "Generate a Report to evaluate the length of each text in the *response* column. Run this check for the first 200 rows of the *qa_logs* dataframe.\n",
    "\n",
    "This calculates the number of symbols in each text and shows a summary. You can see the distribution of the text length across all responses and descriptive statistics like the mean or minimal text length. \n",
    "\n",
    "Click on Details to see how the mean text length changes over time. The index comes from the *datetime* column you mapped earlier. This helps you to notice any temporal patterns, such as if texts are longer or shorter during specific periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_evals_report = Report(metrics=[\n",
    "    TextEvals(column_name=\"response\",\n",
    "              descriptors=[\n",
    "                  TextLength(),\n",
    "                  ]\n",
    "              )\n",
    "])\n",
    "\n",
    "text_evals_report.run(reference_data=None,\n",
    "                      current_data=qa_logs[:200],\n",
    "                      column_mapping=column_mapping)\n",
    "text_evals_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get a side-by-side comparison**. \n",
    "\n",
    "You can also generate statistics for two datasets at once. For example, compare the outputs of two different prompts or data from today against yesterday.\n",
    "\n",
    "Pass one dataset as a reference one and another as a current one. For simplicity, let's compare the text length for the first and next 100 responses from the same dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_evals_report = Report(metrics=[\n",
    "    TextEvals(column_name=\"response\",\n",
    "              descriptors=[\n",
    "                  TextLength(),\n",
    "                  ]\n",
    "              )\n",
    "])\n",
    "\n",
    "text_evals_report.run(reference_data=qa_logs[:100],\n",
    "                      current_data=qa_logs[100:200],\n",
    "                      column_mapping=column_mapping)\n",
    "text_evals_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**\n",
    "\n",
    "Get a side by side comparison of the sentence count for the first and the next 100 rows from the same dataframe. Use SentenceCount() descriptor.\n",
    "Consult the descriptor docs [here](https://docs.evidentlyai.com/reference/all-metrics#descriptors-text-stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_evals_report = Report(metrics=[\n",
    "    TextEvals(column_name=\"response\",\n",
    "              descriptors=[\n",
    "                  ,\n",
    "                  ]\n",
    "              )\n",
    "])\n",
    "\n",
    "text_evals_report.run(reference_data=qa_logs[:100],\n",
    "                      current_data=qa_logs[100:200],\n",
    "                      column_mapping=column_mapping)\n",
    "text_evals_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text patterns\n",
    "\n",
    "Detect specific words or regular patterns by using regular expressions behind the scenes. Such evals are faster and cheaper to compute at scale. \n",
    "\n",
    "For example, check if the responses mention competitors, banned or forbidden words, include emails or other links. Text pattern descriptors return a binary score (\"True\" or \"False\") for pattern matches.\n",
    "\n",
    "Let's check if *responses* contains specific words related to the compensation (such as salary, benefits, or payroll). Pass this word list to the IncludesWords() descriptor. This will also check for word variants. Add an optional display name for this eval.\n",
    "\n",
    "You can see that 18 responses out of 200 relate to the topic of compensation as defined by this word list. Details show occurrences in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_evals_report = Report(metrics=[\n",
    "    TextEvals(column_name=\"response\",\n",
    "              descriptors=[\n",
    "                  IncludesWords(\n",
    "                      words_list=['salary', 'benefits', 'payroll'],\n",
    "                      display_name=\"Mention Compensation\")\n",
    "            ]\n",
    "        ),\n",
    "        ]\n",
    ")\n",
    "\n",
    "text_evals_report.run(reference_data=None,\n",
    "                      current_data=qa_logs[:200],\n",
    "                      column_mapping=column_mapping)\n",
    "text_evals_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "\n",
    "Check whether the first 200 responses contain links or not. Add \"Contains links\" display name for this eval. Find an appropriate descriptor for your task [here](https://docs.evidentlyai.com/reference/all-metrics#descriptors-text-patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_evals_report = Report(metrics=[\n",
    "    TextEvals(column_name=\"response\",\n",
    "              descriptors=[\n",
    "            ]\n",
    "        )\n",
    "        ]\n",
    ")\n",
    "\n",
    "text_evals_report.run(reference_data=,\n",
    "                      current_data=,\n",
    "                      column_mapping=column_mapping)\n",
    "text_evals_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML-based evaluation\n",
    "\n",
    "Uses pre-trained machine learning models for evaluation. \n",
    "\n",
    "Evidently has built-in model-based descriptors and wrappers to call external models published on Hugging Face.\n",
    "\n",
    "**Semantic similarity**\n",
    "\n",
    "You can evaluate how closely two texts are in meaning using an embedding model. SemanticSimilarity() descriptor calculates pairwise semantic similarity between columns for each pair of text. You can compare the text from *Response* and *Question* columns to see if the answers are semantically relevant to the question.\n",
    "\n",
    "SemanticSimilarity() descriptor converts all texts into embeddings, measures Cosine Similarity between them, and returns a score from 0 to 1:\n",
    "* 0 means that texts are opposite in meaning;\n",
    "* 0.5 means that texts are unrelated;\n",
    "* 1 means that texts are semantically close.\n",
    "\n",
    "In this case, the semantic similarity always stays above 0.81, which means that answers generally relate to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_evals_report = Report(metrics=[\n",
    "    TextEvals(column_name=\"response\", descriptors=[\n",
    "        SemanticSimilarity(with_column=\"question\", \n",
    "                           ),\n",
    "    ])\n",
    "])\n",
    "\n",
    "text_evals_report.run(reference_data=None,\n",
    "                      current_data=qa_logs[:200],\n",
    "                      column_mapping=column_mapping)\n",
    "text_evals_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment analysis**\n",
    "\n",
    "Analyzes the sentiment of the text using a word-based model. Returns score on a scale: -1 (negative) to 1 (positive). Shows the distribution of the response sentiment. Allows you to spot specific times when the average sentiment of the responses dipped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**\n",
    "\n",
    "Execute a sentiment check on the first 200 responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_evals_report = Report(metrics=[\n",
    "    TextEvals(column_name=\"response\", descriptors=[\n",
    "            ,\n",
    "        ]\n",
    "    ),\n",
    "])\n",
    "\n",
    "text_evals_report.run(reference_data=,\n",
    "                      current_data=,\n",
    "                      column_mapping=column_mapping)\n",
    "text_evals_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Toxicity**\n",
    "\n",
    "The toxicity measurement aims to quantify the toxicity of the input texts using a pretrained hate speech classification model. In this model, 'hate' is defined as abusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation. This model returns a predicted toxicity score between 0 and 1. First, the descriptor downloads the model from Hugging Face to your environment. Then uses it to score the data. It takes a few moments to load the model. The higher the score the more toxic is your response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_evals_report = Report(metrics=[\n",
    "    TextEvals(column_name=\"response\", descriptors=[\n",
    "            HuggingFaceToxicityModel(),\n",
    "        ]\n",
    "    ),\n",
    "])\n",
    "\n",
    "text_evals_report.run(reference_data=None,\n",
    "                      current_data=qa_logs[:200],\n",
    "                      column_mapping=column_mapping)\n",
    "text_evals_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-as-a-judge\n",
    "\n",
    "For more complex or nuanced checks, you can use LLM-as-a-judge. This requires asking the same or more powerful LLM to assess the text by specific criteria, such as tone or conciseness. Please keep in mind that it is not a precise metric but rather an approximation of human judgment, requiring clear instructions and potentially manual labeling to establish evaluation criteria. This method is well suited for pairwise comparison, direct scoring with reference. \n",
    "\n",
    "**How to create LLM-as-a-judge?**\n",
    "1. Create a test dataset\n",
    "2. Add your own labels and comments (helps to clarify your own criteria)\n",
    "3. Write an eval prompt (use binary or low-precision scores, split complex criteria, ask for reasoning)\n",
    "4. Evaluate the LLM-as-a-judge (to iterate get back to the step 3)\n",
    "5. Deploy the evaluator\n",
    "\n",
    "**Limitations of LLM-as-a-judge**\n",
    "* inconsitency\n",
    "* criteria ambiguity\n",
    "* increased costs and latency\n",
    "* biases of LLM-as-a-judge\n",
    "\n",
    "Evidently has built-in descriptors and prompt templates to create your own custom LLM-as-a-judge (it requires an OpenAI API key). More information could be found in Further resources section at the end of this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata summary\n",
    "\n",
    "The QA dataset has a *feedback* column which includes user upvotes and downvotes. You can easily enrich your Report with summaries from any numerical or categorical columns.\n",
    "\n",
    "Use ColumnSummaryMetric() to add a summary of the *feedback* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_report = Report(metrics=[\n",
    "   ColumnSummaryMetric(column_name=\"feedback\"),\n",
    "   ]\n",
    ")\n",
    "\n",
    "feedback_report.run(reference_data=None, \n",
    "                    current_data=qa_logs[:200], \n",
    "                    column_mapping=column_mapping)\n",
    "feedback_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test suite with evals for a LLM based app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building extensive test suites with evals for a LLM based app takes time. The best way it to start small and extend them gradually with more automated tests.\n",
    "\n",
    "**Where do you need Test suites with evals?**\n",
    "* during development for *regression testing*: run test suites whenever you modify any part of your LLM based app, such as trying a new retrieval strategy, model version, or prompt. The goal is to check that updates don't make the quality worse or introduce new errors. You compare new responses with references or against set of criteria.\n",
    "* in production for *continuous testing*: run test suites periodically over production logs to check that the output quality stays within expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What kind of tests your Test suite can have?**\n",
    "\n",
    "Below you can find some test types, this list is not exhaustive.\n",
    "\n",
    "*Example-based tests*\n",
    "\n",
    "By breaking down the scope of your LLM based app into features and scenarios, you can write assertions for different scenarios. Such tests are usually fast and cheap to run. \n",
    "\n",
    "*Behaviour-driven tests*\n",
    "\n",
    "By writing feature scenarios and expectations in natural language, namely gherkin format (given, when, then), you will be able to align better on the requirements with non-technical stakeholders. For this you can use *behave* (Python OSS library for behavior-driven development).\n",
    "\n",
    "*Adversarial tests*\n",
    "\n",
    "Tests to detect adversarial prompting and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's enough theory for now, let's try it out.\n",
    "\n",
    "So far, you've used Evidently Reports to summarize evaluation outcomes. Now you need to set specific conditions for the metric values to run automated tests (such as check if all texts fall within the expected length range) and review results. This is where you can use an alternative interface called **TestSuites**. TestSuites work similarly to Reports, but instead of listing metrics, you define tests and set conditions using parameters like gt (greater than), lt (less than), eq (equal), etc. \n",
    "\n",
    "**Define a Test Suite**\n",
    "\n",
    "Add tests to check the following conditions:\n",
    "* response length is always non-zero\n",
    "* maximum response length does not exceed 1800 symbols (e.g., due to UI constraints).\n",
    "* mean response length is above 500 symbols (e.g., this is a known pattern)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_suite = TestSuite(tests=[\n",
    "    TestColumnValueMin(column_name = TextLength().on(\"response\"), gt=0),\n",
    "    TestColumnValueMax(column_name = TextLength().on(\"response\"), lte=1800),\n",
    "    TestColumnValueMean(column_name = TextLength().on(\"response\"), gt=500),\n",
    "])\n",
    "\n",
    "test_suite.run(reference_data=None, \n",
    "                    current_data=qa_logs[:200], \n",
    "                    column_mapping=column_mapping)\n",
    "test_suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**\n",
    "\n",
    "Enrich the current test suite with a new condition: average response sentiment is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_suite = TestSuite(tests=[\n",
    "    TestColumnValueMin(column_name = TextLength().on(\"response\"), gt=0),\n",
    "    TestColumnValueMax(column_name = TextLength().on(\"response\"), lte=1800),\n",
    "    TestColumnValueMean(column_name = TextLength().on(\"response\"), gt=500),\n",
    "    ,\n",
    "])\n",
    "\n",
    "test_suite.run(reference_data=None, \n",
    "                    current_data=qa_logs[:200], \n",
    "                    column_mapping=column_mapping)\n",
    "test_suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Test Suite**\n",
    "\n",
    "You can start by re-using available tests presets, later you can design a custom Test Suite by picking up specific Tests and setting conditions more precisely. \n",
    "1. Choose individual Tests: select the tests you want to include in your Test Suite.\n",
    "2. Pass Test parameters: set custom parameters for applicable Tests\n",
    "3. Set custom conditions: define when Tests should pass or fail.\n",
    "4. Mark Test criticality: mark non-critical Tests to give a Warning instead of Fail. \n",
    "\n",
    "More extended information can be found [here](https://docs.evidentlyai.com/user-guide/tests-and-reports/run-tests#custom-test-suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring foundations for a LLM based app\n",
    "\n",
    "**Observability** provides a comprehensive view of your LLM based app performance beyond predefined metrics. The more complex your LLM based app is, the more critical it becomes to have a proper observability. \n",
    "\n",
    "**How monitoring is related to evaluation?**\n",
    "\n",
    "As a part of your observability system, both *monitoring* and *evaluation* share a common objective: to minimize potential risks (such as app failures, security attacks, and drifts) and identify opportunities for enhancing app performance and reducing costs. Evaluation metrics should be translated into monitoring metrics. Issues detected during monitoring should be incorporated back into your evaluation process. \n",
    "\n",
    "**Metrics as the backbone of your monitoring system**\n",
    "\n",
    "Before deciding which metrics to track, identify the potential failures you want to detect and then design metrics specifically to catch those failures. Format errors are the simplest failures to start with. Length-related metrics will help you to track latency and costs, as longer contexts and responses typically increase latency and incur higher costs. Latency metrics can be time to first token, time per output token, total response latency. Cost-related metrics can be number of queries, tokens per second. For safety, you can track toxicity and detect sensitive information in both inputs and outputs, the frequency of guardrail triggers and refusals to answer. Also, keep an eye on unusual queries, as these can highlight edge cases or potential attacks. Keep in mind that each component in your LLM based app will come with its own set of specific metrics. When calculating metrics, ensure they can be segmented by key dimensions like users, prompt or chain versions and types, and time. This detailed breakdown will help you to understand performance variations and identify specific issues.\n",
    "\n",
    "**Your typical production debbuging workflow**\n",
    "1. Your metrics signal an issue, but don't explain its cause. \n",
    "2. To understand the root cause you examine logs from the time the metrics indicated the issue. \n",
    "3. You correlate the errors found in those logs to the metrics to confirm the correct issue has been identified.\n",
    "\n",
    "**Why metrics and logs are not enough for LLM based apps?**\n",
    "\n",
    "Having *metrics* and *logs* is not enough for LLM based apps if you want to trace each request step-by-step through the system. This requires *tracing*, the detailed recording of a request’s execution path through various system components and services. It reveals the entire process from when a user sends a query to when the final response is returned to the user, including the actions the system takes, the documents retrieved, and the final prompt sent to the model. Additionaly, if measurable, it can show how much time each step took and its associated cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to play around with tracing in Evidently. It uses *tracely* - Python OSS library based on OpenTelemetry to track events in your LLM based apps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding remarks\n",
    "\n",
    "Main take-aways:\n",
    "* evaluation-driven development steers development of LLM based apps to the right direction\n",
    "* without proper evaluation framework your LLM based app is doomed\n",
    "* automated test suits help you to bring your evaluation framework to life\n",
    "* right combination of metrics, logs and tracing not only helps you sleep during the night, but also increase compliance of your AI system with the EU AI Act  \n",
    "\n",
    "**Further resources**\n",
    "\n",
    "* [What We’ve Learned From A Year of Building with LLMs](https://applied-llms.org/)\n",
    "* [Patterns for Building LLM-based Systems & Products](https://eugeneyan.com/writing/llm-patterns/#evals-to-measure-performance)\n",
    "* [Tutorial: LLM Regression testing](https://docs.evidentlyai.com/tutorials-and-examples/cookbook_llm_regression_testing)\n",
    "* [Levels of Complexity: RAG Applications](https://jxnl.github.io/blog/writing/2024/02/28/levels-of-complexity-rag-applications/)\n",
    "* [Creating a LLM-as-a-Judge That Drives Business Results](https://hamel.dev/blog/posts/llm-judge/)\n",
    "* [Tutorial: LLM-as-a-judge](https://docs.evidentlyai.com/tutorials-and-examples/cookbook_llm_judge)\n",
    "* [Adversarial Prompting in LLMs](https://www.promptingguide.ai/risks/adversarial)\n",
    "* [Monitoring overview](https://docs.evidentlyai.com/user-guide/monitoring/monitoring_overview)\n",
    "* [Tutorial: Tracing](https://docs.evidentlyai.com/tutorials-and-examples/tutorial_tracing)\n",
    "\n",
    "\n",
    "**Used materials**\n",
    "\n",
    "1. [LLM Evaluation course by Evidently](https://youtube.com/playlist?list=PL9omX6impEuMgDFCK_NleIB0sMzKs2boI&feature=shared)\n",
    "2. [LLM Evaluation tutorial by Evidently](https://docs.evidentlyai.com/tutorials-and-examples/tutorial-llm)\n",
    "3. [AI Engineering book by Chip Huyen](https://learning.oreilly.com/library/view/ai-engineering/9781098166298/)\n",
    "4. [Your AI Product Needs Evals by Hamel Husain](https://hamel.dev/blog/posts/evals/)\n",
    "5. [Evaluation-driven development workflow by Databricks](https://docs.databricks.com/en/generative-ai/tutorials/ai-cookbook/evaluation-driven-development.html)\n",
    "6. [LLM-workshop by MLOps and Crafts](https://github.com/mlops-and-crafts/llm-workshop/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
