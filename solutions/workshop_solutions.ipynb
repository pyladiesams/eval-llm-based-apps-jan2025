{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop solutions for Evaluating LLM based applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation framework for a LLM based app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and dataset prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from evidently import ColumnMapping\n",
    "from evidently.descriptors import *\n",
    "from evidently.metric_preset import TextEvals\n",
    "from evidently.metrics import *\n",
    "from evidently.report import Report\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.tests import *\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://raw.githubusercontent.com/pyladiesams/eval-llm-based-apps-jan2025/main/assets/QA.csv\")\n",
    "qa_csv_content = BytesIO(response.content)\n",
    "qa_logs = pd.read_csv(qa_csv_content, index_col=0, parse_dates=['start_time', 'end_time'])\n",
    "qa_logs.index = qa_logs.start_time\n",
    "qa_logs.index.rename('index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = ColumnMapping(\n",
    "    datetime='start_time',\n",
    "    datetime_features=['end_time'],\n",
    "    text_features=['question', 'response'],\n",
    "    categorical_features=['organization', 'model_ID', 'region', 'environment', 'feedback'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1: Solution**\n",
    "\n",
    "Get a side by side comparison of the sentence count for the first and the next 100 rows from the same dataframe. Use SentenceCount() descriptor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_evals_report = Report(metrics=[\n",
    "    TextEvals(column_name=\"response\",\n",
    "              descriptors=[\n",
    "                  SentenceCount(),\n",
    "                  ]\n",
    "              )\n",
    "])\n",
    "\n",
    "text_evals_report.run(reference_data=qa_logs[:100],\n",
    "                      current_data=qa_logs[100:200],\n",
    "                      column_mapping=column_mapping)\n",
    "text_evals_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2: Solution**\n",
    "\n",
    "Check whether the first 200 responses contain links or not. Add \"Contains links\" display name for this eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_evals_report = Report(metrics=[\n",
    "    TextEvals(column_name=\"response\",\n",
    "              descriptors=[\n",
    "                  ContainsLink(\n",
    "                      display_name=\"Contains links\")\n",
    "            ]\n",
    "        )\n",
    "        ]\n",
    ")\n",
    "\n",
    "text_evals_report.run(reference_data=None,\n",
    "                      current_data=qa_logs[:200],\n",
    "                      column_mapping=column_mapping)\n",
    "text_evals_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML-based evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentiment analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3: Solution** \n",
    "\n",
    "Execute a sentiment check on the first 200 responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_evals_report = Report(metrics=[\n",
    "    TextEvals(column_name=\"response\", descriptors=[\n",
    "            Sentiment(),\n",
    "        ]\n",
    "    ),\n",
    "])\n",
    "\n",
    "text_evals_report.run(reference_data=None,\n",
    "                      current_data=qa_logs[:200],\n",
    "                      column_mapping=column_mapping)\n",
    "text_evals_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test suite with evals for a LLM based app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4: Solution**\n",
    "\n",
    "Enrich the current test suite with a new condition: average response sentiment is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_suite = TestSuite(tests=[\n",
    "    TestColumnValueMin(column_name = TextLength().on(\"response\"), gt=0),\n",
    "    TestColumnValueMax(column_name = TextLength().on(\"response\"), lte=1800),\n",
    "    TestColumnValueMean(column_name = TextLength().on(\"response\"), gt=500),\n",
    "    TestColumnValueMean(column_name = Sentiment().on(\"response\"), gte=0),\n",
    "])\n",
    "\n",
    "test_suite.run(reference_data=None, \n",
    "                    current_data=qa_logs[:200], \n",
    "                    column_mapping=column_mapping)\n",
    "test_suite"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
